{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Import & File Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-02T08:35:57.820203Z",
     "iopub.status.busy": "2023-08-02T08:35:57.819819Z",
     "iopub.status.idle": "2023-08-02T08:36:03.149546Z",
     "shell.execute_reply": "2023-08-02T08:36:03.148448Z",
     "shell.execute_reply.started": "2023-08-02T08:35:57.820172Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paper About Transformer: https://arxiv.org/pdf/1706.03762.pdf <br>\n",
    "- The Transformer code is adapted from: https://www.youtube.com/watch?v=U0s0f995w14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention (Scaled Dot-Product Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:03.152634Z",
     "iopub.status.busy": "2023-08-02T08:36:03.151719Z",
     "iopub.status.idle": "2023-08-02T08:36:03.166273Z",
     "shell.execute_reply": "2023-08-02T08:36:03.164873Z",
     "shell.execute_reply.started": "2023-08-02T08:36:03.152580Z"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        \n",
    "        assert (self.head_dim * heads == embed_size), \"Embed Size needs to be div by heads\"\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "        \n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        \n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  #E = QK^T\n",
    "        #queries shape: (N, query_len, heads, heads_dim), ......\n",
    "        \n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask==0, float(\"-1e20\"))   #-infinity\n",
    "        \n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "        \n",
    "        out = torch.einsum(\"nhqk,nkhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads*self.head_dim\n",
    "        )\n",
    "        #attention shape: (N, heads, query_len, key_len)\n",
    "        #values shape: (N, value_len, heads, heads_dim)\n",
    "        # (N, query_len, heads, head_dim) (key_len = query_len)\n",
    "        out  = self.fc_out(out)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:03.169563Z",
     "iopub.status.busy": "2023-08-02T08:36:03.168724Z",
     "iopub.status.idle": "2023-08-02T08:36:03.191269Z",
     "shell.execute_reply": "2023-08-02T08:36:03.190280Z",
     "shell.execute_reply.started": "2023-08-02T08:36:03.169525Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention.forward(value, key, query, mask)\n",
    "            \n",
    "        x = self.dropout(self.norm1(attention+query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward+x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:03.194915Z",
     "iopub.status.busy": "2023-08-02T08:36:03.194509Z",
     "iopub.status.idle": "2023-08-02T08:36:03.207899Z",
     "shell.execute_reply": "2023-08-02T08:36:03.206793Z",
     "shell.execute_reply.started": "2023-08-02T08:36:03.194880Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length    #Avoid Sentences are too long\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        print(\"embed_size\", embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N,seq_length).to(self.device)\n",
    "            \n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "        \n",
    "                \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:03.210107Z",
     "iopub.status.busy": "2023-08-02T08:36:03.209621Z",
     "iopub.status.idle": "2023-08-02T08:36:03.226035Z",
     "shell.execute_reply": "2023-08-02T08:36:03.225039Z",
     "shell.execute_reply.started": "2023-08-02T08:36:03.210076Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion, \n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x,x,x,trg_mask)\n",
    "        query = self.dropout(self.norm(attention+x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:03.227959Z",
     "iopub.status.busy": "2023-08-02T08:36:03.227543Z",
     "iopub.status.idle": "2023-08-02T08:36:03.240283Z",
     "shell.execute_reply": "2023-08-02T08:36:03.239249Z",
     "shell.execute_reply.started": "2023-08-02T08:36:03.227924Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 trg_vocab_size,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length\n",
    "                ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x)+self.position_embedding(positions)))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x,enc_out, enc_out, src_mask, trg_mask)\n",
    "            \n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer (Whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:03.241726Z",
     "iopub.status.busy": "2023-08-02T08:36:03.241384Z",
     "iopub.status.idle": "2023-08-02T08:36:03.257175Z",
     "shell.execute_reply": "2023-08-02T08:36:03.256338Z",
     "shell.execute_reply.started": "2023-08-02T08:36:03.241701Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=256,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device='cuda',\n",
    "        max_length=100\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length\n",
    "        )\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #(N,1,1,src_len)\n",
    "        return src_mask.to(self.device)\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len,trg_len))).expand(\n",
    "            N,1,trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:03.259339Z",
     "iopub.status.busy": "2023-08-02T08:36:03.258606Z",
     "iopub.status.idle": "2023-08-02T08:36:03.408773Z",
     "shell.execute_reply": "2023-08-02T08:36:03.407733Z",
     "shell.execute_reply.started": "2023-08-02T08:36:03.259307Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries_train_df=pd.read_csv(\"input/summaries_train.csv\")\n",
    "summaries_test_df=pd.read_csv(\"input/summaries_test.csv\")\n",
    "prompts_train_df=pd.read_csv(\"input/prompts_train.csv\")\n",
    "prompts_test_df=pd.read_csv(\"input/prompts_test.csv\")\n",
    "sample_submission_df=pd.read_csv(\"input/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:03.411026Z",
     "iopub.status.busy": "2023-08-02T08:36:03.410382Z",
     "iopub.status.idle": "2023-08-02T08:36:03.458669Z",
     "shell.execute_reply": "2023-08-02T08:36:03.457557Z",
     "shell.execute_reply.started": "2023-08-02T08:36:03.410993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"val Problems:\",len(val_problems))\\nprint(\"val Answers:\",len(val_answers))\\nprint(\"val Scores:\",scores)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "problems_train = prompts_train_df.values.tolist()\n",
    "train_set = summaries_train_df[['prompt_id','text','content','wording']].values.tolist()\n",
    "train_set = train_set[:100]   #Now, just use 100 training data, 0: prompt_id, 1: text, 2: content, 3:wording, 4: problems\n",
    "val_set = train_set[-20:]   #Now, just use 100 training data, 0: prompt_id, 1: text, 2: content, 3:wording, 4: problems\n",
    "#Append problem in train_set\n",
    "problems = []\n",
    "answers = []\n",
    "scores = []\n",
    "padding_length = 300\n",
    "for sheet in train_set:\n",
    "    for problem in problems_train:\n",
    "        if sheet[0] == problem[0]:\n",
    "            sheet.append(problem[1])\n",
    "            break\n",
    "    sentence = sheet[4].split(\" \")\n",
    "    if len(sentence) > padding_length:\n",
    "        print(\"length\", len(sentence))\n",
    "        print(\"Senetence\",sentence)\n",
    "        print(\"Length is bigger then padding_length!!\")\n",
    "        sys.exit(1)\n",
    "    for _ in range(padding_length-len(sentence)):\n",
    "        sentence.append(\"0\")\n",
    "    problems.append(sentence)\n",
    "    sentence = sheet[1].split(\" \")\n",
    "    if len(sentence) > padding_length:\n",
    "        print(\"length\", len(sentence))\n",
    "        print(\"Sentence\", sentence)\n",
    "        print(\"Length is bigger then padding_length!\")\n",
    "        sys.exit(1)\n",
    "    for _ in range(padding_length-len(sentence)):\n",
    "        sentence.append(\"0\")\n",
    "    answers.append(sentence)\n",
    "    scores.append(sheet[2:4])\n",
    "\n",
    "val_problems = []\n",
    "val_answers = []\n",
    "val_scores = []\n",
    "for sheet in val_set:\n",
    "    for problem in problems_train:\n",
    "        if sheet[0] == problem[0]:\n",
    "            sheet.append(problem[1])\n",
    "            break\n",
    "    sentence = sheet[4].split(\" \")\n",
    "    if len(sentence) > padding_length:\n",
    "        print(\"length\", len(sentence))\n",
    "        print(\"Senetence\",sentence)\n",
    "        print(\"Length is bigger then padding_length!!\")\n",
    "        sys.exit(1)\n",
    "    for _ in range(padding_length-len(sentence)):\n",
    "        sentence.append(\"0\")\n",
    "    val_problems.append(sentence)\n",
    "    sentence = sheet[1].split(\" \")\n",
    "    if len(sentence) > padding_length:\n",
    "        print(\"length\", len(sentence))\n",
    "        print(\"Sentence\", sentence)\n",
    "        print(\"Length is bigger then padding_length!\")\n",
    "        sys.exit(1)\n",
    "    for _ in range(padding_length-len(sentence)):\n",
    "        sentence.append(\"0\")\n",
    "    val_answers.append(sentence)\n",
    "    val_scores.append(sheet[2:4])\n",
    "\n",
    "\"\"\"\n",
    "print(\"Problems:\",problems)\n",
    "print(\"Answers:\",answers)\n",
    "print(\"Scores:\",scores)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "print(\"val Problems:\",len(val_problems))\n",
    "print(\"val Answers:\",len(val_answers))\n",
    "print(\"val Scores:\",scores)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:03.463825Z",
     "iopub.status.busy": "2023-08-02T08:36:03.462820Z",
     "iopub.status.idle": "2023-08-02T08:36:04.746687Z",
     "shell.execute_reply": "2023-08-02T08:36:04.745622Z",
     "shell.execute_reply.started": "2023-08-02T08:36:03.463788Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "all_words = sum(problems,[])+sum(answers,[])+sum(val_problems, [])+sum(val_answers, [])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "temp = label_encoder.fit_transform(all_words)\n",
    "max_encoded_value = np.max(temp)\n",
    "for i in range(len(train_set)):\n",
    "    problems[i] = label_encoder.transform(problems[i]).tolist()\n",
    "    answers[i] = label_encoder.transform(answers[i]).tolist()\n",
    "    \n",
    "for i in range(len(val_set)):\n",
    "    val_problems[i] = label_encoder.transform(val_problems[i]).tolist()\n",
    "    val_answers[i] = label_encoder.transform(val_answers[i]).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:04.748908Z",
     "iopub.status.busy": "2023-08-02T08:36:04.748205Z",
     "iopub.status.idle": "2023-08-02T08:36:04.755165Z",
     "shell.execute_reply": "2023-08-02T08:36:04.753786Z",
     "shell.execute_reply.started": "2023-08-02T08:36:04.748869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Src_vocab_size 1808\n"
     ]
    }
   ],
   "source": [
    "print(\"Src_vocab_size\", np.max(temp)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea of the Model: \n",
    "- Only use encoder: <br>\n",
    "Problems -> Encoder -> enc_pro\n",
    "                                  -> Neuron Network -> out <- MSE Loss -> true_scores\n",
    "Answers -> Encoder -> enc->ans <br>\n",
    "- Call the total process: Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:04.757871Z",
     "iopub.status.busy": "2023-08-02T08:36:04.756960Z",
     "iopub.status.idle": "2023-08-02T08:36:04.776067Z",
     "shell.execute_reply": "2023-08-02T08:36:04.775149Z",
     "shell.execute_reply.started": "2023-08-02T08:36:04.757830Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Measure(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        word_length,\n",
    "        src_pad_idx,\n",
    "        embed_size=256,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device='cuda',\n",
    "        max_length=500\n",
    "        ):\n",
    "        super(Measure, self).__init__()\n",
    "        self.problem = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "        self.answer = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "        self.word_length = word_length\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.word_length*(embed_size+embed_size), 2*forward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*forward_expansion*embed_size, embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size, 2)\n",
    "        )\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #(N,1,1,src_len)\n",
    "        return src_mask.to(self.device)\n",
    "    def forward(self, pro, ans):\n",
    "        pro_mask = self.make_src_mask(pro)\n",
    "        ans_mask = self.make_src_mask(ans)\n",
    "        pro_src = self.problem(pro, pro_mask)\n",
    "        ans_src = self.answer(ans, ans_mask)\n",
    "        #ans_src = ans_src.unsqueeze(1)\n",
    "        \n",
    "        x = torch.cat((pro_src, ans_src), dim=1)\n",
    "        x = x.reshape(x.shape[0], x.shape[1]*x.shape[2])\n",
    "        out = self.feed_forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config & Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using: cpu\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = max_encoded_value+1\n",
    "num_layers = 3\n",
    "src_pad_idx = 0\n",
    "word_length = padding_length\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 2\n",
    "dropout = 0\n",
    "embed_size = 256\n",
    "print(\"Now using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T08:36:04.777387Z",
     "iopub.status.busy": "2023-08-02T08:36:04.777105Z",
     "iopub.status.idle": "2023-08-02T08:36:27.862718Z",
     "shell.execute_reply": "2023-08-02T08:36:27.861732Z",
     "shell.execute_reply.started": "2023-08-02T08:36:04.777350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_size 256\n",
      "embed_size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/jjkpr9w94h55brqcv4fw9ybm0000gn/T/ipykernel_2192/2980984210.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_problems = torch.tensor(problems).to(device)\n",
      "/var/folders/gn/jjkpr9w94h55brqcv4fw9ybm0000gn/T/ipykernel_2192/2980984210.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_answers = torch.tensor(answers).to(device)\n",
      "/var/folders/gn/jjkpr9w94h55brqcv4fw9ybm0000gn/T/ipykernel_2192/2980984210.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_scores = torch.tensor(scores).to(device)\n"
     ]
    }
   ],
   "source": [
    "problems = torch.tensor(problems).to(device)\n",
    "answers = torch.tensor(answers).to(device)\n",
    "scores = torch.tensor(scores).to(device)\n",
    "val_problems = torch.tensor(problems).to(device)\n",
    "val_answers = torch.tensor(answers).to(device)\n",
    "val_scores = torch.tensor(scores).to(device)\n",
    "#problems = torch.tensor([[1,5,6,4,3,9,5,2,0], [1,8,7,3,4,5,6,7,2]]).to(device)\n",
    "#answers = torch.tensor([[1,7,4,2,3,5,9,2,0],[1,5,6,1,2,4,7,6,2]]).to(device)\n",
    "#scores = torch.tensor([[0.2, 0.3], [0.1, 0.2]]).to(device)\n",
    "model = Measure(src_vocab_size,word_length,src_pad_idx,embed_size=embed_size,num_layers=num_layers,device=device, dropout=dropout).to(device)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "for i in range(epochs):\n",
    "    out = model(problems, answers)\n",
    "    loss = loss_fn(out, scores)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(\"Loss of training \"+str(i)+\": \", loss.item())\n",
    "    with torch.no_grad():\n",
    "        out = model(val_problems, val_answers)\n",
    "        val_loss = loss_fn(out, val_scores)\n",
    "        print(\"Loss of valid \"+str(i)+\": \", val_loss.item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
